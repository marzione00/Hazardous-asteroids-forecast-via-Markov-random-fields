% !TEX TS-program = pdflatex
% !TEX root = Tes.tex
% !TEX spellcheck = en-EN

\documentclass[12pt,%                      % corpo del font principale
               a4paper,%                   % A4 papers
               %twoside,openright,%         % twoside with free right side
               oneside,openany,%           % one side
               titlepage,%                 % use a titlepage
               headinclude,footinclude,%  % header and foot header
               BCOR5mm,%                   % rilegatura di 5 mm
               cleardoublepage=empty,%     % empty pages with no header and foot
               tablecaptionabove,%         % table caption above tables
               floatperchapter,
               ]{scrreprt}                 % KOMA-Script report class;


\usepackage{braket}
\usepackage{changepage}
\usepackage[english]{babel}	% latest language is predefined
\usepackage[T1]{fontenc}		% font coding


\usepackage{indentfirst}		% indent first paragraph of each section
\usepackage{mparhack,fixltx2e,relsize}	% fancy typographies stuff

\usepackage[eulerchapternumbers,%	% chapter font Euler
            subfig,%			% in subfig objects
            beramono,%			% Bera Mono as fixed spacing font
            eulermath,%			% AMS Euler as math font
            pdfspacing,%		% improves line filling
            listings,%			% code output
 %          parts,%			% uncomment for a document divided in parts
            listsseparated,
            ]{classicthesis}		% style ClassicThesis
%\setlength{\cftbeforeloftitleskip}{100pt}
%\renewcommand{\cftbeforeloftitleskip}{1000pt}

\usepackage{arsclassica}		% modifies some aspects of ClassicThesis package
\let\marginpar\oldmarginpar		% for margin notes with \todonotes (overwise conflict with the new definition of \marginpar in classic thesis)
\usepackage[shadow]{todonotes}			% for margin notes and comments

\usepackage{bookmark}			% bookmarks

%*********************************************************************************
% Bibliography
%*********************************************************************************
%%\usepackage[style=authoryear,hyperref,backref,natbib, ,maxcitenames=1, mincitenames = 1, citestyle=authoryear-comp, backend=biber,sortcites,sorting=ynt]{biblatex}
%\usepackage[style=numeric,hyperref,backref,natbib]{biblatex}




%*********************************************************************************
% Graphics
%*********************************************************************************
\usepackage{graphicx}			% images
\usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{tikz}
\usetikzlibrary{mindmap,trees}
\usetikzlibrary{backgrounds}
\usepackage{verbatim}
\usepackage[dvipsnames]{xcolor}
% \usepackage{morefloats}
%\usepackage{chngcntr}
\usepackage{pdfpages}
\usepackage{braket}
\usepackage{amssymb}
\def\delequal{\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}}
\usepackage{lscape}


%*********************************************************************************
% Tables
%*********************************************************************************
\usepackage{tabularx}			% table of predefined length
\usepackage{siunitx}
\usepackage{pbox}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{rotating}
\usepackage{hhline}
\setlength\tabcolsep{3pt}
\usepackage{changepage}
% \usepackage[showframe=true]{geometry}

%*********************************************************************************
% Mathematics and symbols
%*********************************************************************************
\usepackage{amsmath, amssymb, amsthm}	% mathematics stuff
\usepackage{mathrsfs}
\usepackage{calc}
\usepackage{algorithmic}
\usepackage[ruled]{algorithm}
\usepackage{latexsym}
\usepackage[geometry]{ifsym}
\usepackage{mathabx}
\usepackage{pifont}

%*********************************************************************************
% Personal
%*********************************************************************************
\usepackage[font=itshape]{quoting}			% fancy quotation packages. [font=small] old option
\usepackage[english]{varioref}		% complete reference package
\usepackage{hyperref}
\usepackage{url}
\usepackage[intoc, english, noprefix]{nomencl}	%for list of symbols
\usepackage[normalem]{ulem}
\usepackage{chemfig} %for chemical formulas
\usepackage{eurosym}			% euro symbol
\usepackage{epigraph}
\usepackage{calligra}
\usepackage{soul}
\usepackage[utf8]{inputenc}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}


\setlength{\epigraphwidth}{\textwidth}



%*********************************************************************************
% Calling personal settings and making nomenclature
%*********************************************************************************
\input{custom-commands}
\input{general-settings}  % general custom settings (margins etc)

% \makenomenclature
% \renewcommand{\nomname}{List of Symbols and Abbreviations}

\bibliographystyle{plain}

\begin{document}



%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR(S)
%----------------------------------------------------------------------------------------

\title{\normalfont\spacedallcaps{Hazardous asteroids forecast via Markov random fields}} % The article title

\subtitle{Project for the course Probabilistic modelling (DSE)} % Uncomment to display a subtitle

\author{
  Marzio De Corato
}

\date{} % An optional date to appear under the author(s)

%----------------------------------------------------------------------------------------


%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LISTS OF FIGURES AND TABLES
%----------------------------------------------------------------------------------------

\maketitle % Print the title/author/date block

%\setcounter{tocdepth}{2} % Set the depth of the table of contents to show sections and subsections only



%\listoffigures % Print the list of figures%

%\listoftables % Print the list of tables

\newpage

\epigraph{
\textit{This day may possibly be my last: but the laws of probability, so true in general, so fallacious in particular, still allow about fifteen years. }\\Edward Gibbon (1737-1794)
}

\newpage

\section*{\begin{center}
Abstract
\end{center}}




\newpage

\tableofcontents % Print the table of contents

\newpage % Start the article content on the second page, remove this if you have a longer abstract that goes onto the second page



\newpage
%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\chapter{Introduction}

\chapter{Theoretical Framework}

In this section we are going to review the theoretical concepts that underlies to the probabilistic methods here used:  we will expose them following the approaches of Murphy \cite{murphy2012machine},  Koller et al. \cite{koller2009probabilistic}, H{\o}jsgaardand \cite{hojsgaard2012graphical} et al.  and Russel et al. \cite{russell2010artificial}.  Furthermore we will provide also a rapid overview of the main concepts of information theory,  following the Cover \cite{cover2006elements} and MacKay \cite{mackay2003information} approaches ,  since we used some of its concepts in the preliminary analysis of the dataset.  On the other side the concepts related to the celestial mechanics here used will be described, following the Murray approach \cite{murray1999solar} into the Appendix A

\section{Markov random fields}

Lets start by supposing that we would represent compactly a joint distribution such as \cite{murphy2012machine}:

\begin{equation}
p(x_{1},x_{2},...,x_{n})
\end{equation}

that can represent for instance words in a documents or pixels of an image.  Firstly we know that using the chain rule,  we can decompose it, into the following form \cite{murphy2012machine}:

\begin{equation}
p(x_{1:V})=p(x_{1})p(x_{2}|x_{1})p(x_{3}|x_{2},x_{1})...p(x_{V}|x_{1:V-1})
\end{equation}

where V is the number of variables and 1:V stands for ${1,2,...,V}$.  This decomposition makes explicit the conditional probability tables, or in other terms the transition probability tensors \cite{wu2017markov}.  As one can point out the number of parameter is cumbersome as the number of variables grows: indeed the number of parameter required scales as $\mathcal{O}(K^{V})$. 
Such formidable problem can be attacked by considering the concept of conditional independence.  This is defined as  \cite{murphy2012machine}:

\begin{equation}
X  \perp Y| Z \iff  p(X,Y|Z) = p(X|Z)p(Y|Z)
\end{equation}

A particular case of this definition is the Markov assumption,  by which \textit{the future is independent from the past given the present } or in symbols \cite{murphy2012machine}: 

\begin{equation}
p(\textbf{x}_{1:V})=p(x_{1})\prod^{V}_{t=1}p(x_{t}|x_{t-1})
\end{equation}

In this case a first order Markov chain is obtained,  where the transition tensor is of second order \cite{wu2017markov}.  Given this formalism we are interested in finding a smart way to plot such joint distribution into an intuitive way: the graph theory provide the answer to this quest.  In particular the random variables can be represented by nodes and presence of conditional independence for two random variables by the lack of an edge that interconnects them.  Bayesian networks consider directed edges,  while Markov random fields (MRF) only undirected.  As consequence,  while the the concept of topological ordering,  by which the parents n nodes are labelled with a lower  with respect to their children,  is well defined for Bayesian network,  for MRF is not.  In order to solve this issue it is useful to consider the Hammersley-Clifford theorem as stated in \cite{murphy2012machine}:

\begin{theorem}[Hammersley-Clifford]
A positive distribution p(\textbf{y})>0 satisfies the CI properties of an indirect graph G iif p can be represented as a product of factor, one per maximal clique,  i.e.
\begin{equation}
p(\textbf{y}|\theta)= \dfrac{1}{Z(\theta)}\prod_{c \in C }\psi_{c}(\textbf{y}_{c}|\theta_{c})
\end{equation}
where C is the set of all the (maximal) cliques of G,  and Z($\theta$) is the partition function given by 
\begin{equation}
Z(\theta):= \sum_{y}\prod_{c\in C}\psi_{c}(\textbf{y}_{c}|\theta_{c})
\end{equation}
Note that this partition function is what ensures the overall distribution sums to 1
\end{theorem}

Such theorem allows to represent a probability distribution with potential functions for each maximal clique in the graph.  A particular case of these is the Gibbs distribution \cite{murphy2012machine}: 

\begin{equation}
p(y|\theta)=\dfrac{1}{Z(\theta)} exp\left(-\sum_{c}E(y_{c}|\theta_{c})\right)
\end{equation}

where $E(y_{c})>0$ represent the energy associated with the variables in the clique c.  This form can be adapted to a UGM with the following expression \cite{murphy2012machine}:

\begin{equation}
\psi_{c}(y_{c}|\theta_{c})=exp\left(-E(y_{c}|\theta_{c})\right)
\end{equation}

Finally in order to reduce the computational cost,  one can consider only the pairwise interaction instead of the maximum clique. This is the analogue of what is usually performed in solid state physics (but surely not always)  when only the interaction between the first neighbour is considered.  Another example is the  Ising model: here we have a lattice of spins that can be or in $\ket{+}$ or in $\ket{-}$ and their interaction is modelled by\cite{murphy2012machine}:


\begin{equation}
\psi_{st}\left(y_{s},y_{t}\right) =
\begin{pmatrix}
e^{w_{st}} & e^{-w_{st}} \\
e^{-w_{st}} & e^{w_{st}} \\
\end{pmatrix}
\end{equation}

where $w_{st}=J$ represent the coupling strength between two neighbour site.  The collective state is described by 

\begin{equation}
\ket{i_{1},i_{2},...,i_{n}}=\ket{i_{1}}\otimes\ket{i_{2}}\otimes...\otimes\ket{i_{n}}
\end{equation}

where $\otimes$ is the tensor product.  If this parameter is associated with a positive finite value we have an associative Markov network: basically collective states in which all sites have the same configuration is favoured. Thus we will have two collective states: one for which we have all $\ket{+}$ and another in which we have all $\ket{-}$  Such situation would model,  in principle,  the ferromagnet materials where the external magnetic field induce into the material a magnetic filed with the same direction.  On the other side if the magnetization of the material is opposite with respect to the external field,  and thus $J<0$,  we have an anti-ferromagnetic system in which frustrated states are present.  Furthermore lets consider the unnormalized log probability of a collective state $\textbf{y}=\ket{i_{1},i_{2},...,i_{n}}$ \cite{murphy2012machine}:

\begin{equation}
\log\tilde{\textbf{p}}(y)= -\sum_{s\sim t}y_{s}w_{st}y_{t}
\end{equation}

If we also consider an external field \cite{murphy2012machine}:

\begin{equation}
\log\tilde{\textbf{p}}(y)= -\sum_{s\sim t}y_{s}w_{st}y_{t}+\sum_{s}b_{s}y_{s}
\end{equation}

But this is nothing more that the well know \footnote{In physics} Hamiltonian of an Ising system. This is not a simple coincidence:  indeed the Hamiltonian of a system represent,  rudely speaking,  its total energy.  Thus according to the Boltzmann or Gibbs distribution we have \cite{murphy2012machine}:

\begin{equation}
P_{\beta}(\textbf{y})=\dfrac{e^{-\beta H(\textbf{y}})}{Z_{\beta}}
\end{equation}

where $\beta$ is proportional to the inverse of the system temperature.  Coming back the unnormalized probability of a collective state $\textbf{y}$,  if we set $\Sigma^{-1}=\textbf{W}$,  $\boldsymbol{\mu}=\boldsymbol{\Sigma} \textbf{b}$ and $c=\dfrac{1}{2}\mu^{T}\boldsymbol{\Sigma}^{-1}\mu$ we obtain a Gaussian \cite{murphy2012machine}:

\begin{equation}
\tilde{\textbf{p}}(y)\sim exp\left( -\frac{1}{2} (\textbf{y}-\boldsymbol{\mu})^{T} \boldsymbol{\Sigma}^{-1} (\textbf{y}-\boldsymbol{\mu}) + c \right)
\end{equation}

In general we refer to Gaussian Markov random fields for a joint distribution that can be decomposed in the following way \cite{murphy2012machine}:

\begin{equation}
p\left(\textbf{y}|\boldsymbol{\theta}\right) \propto \prod_{s\sim t} \psi_{st}\left(y_{s},y_{t}\right)\prod_{t}\psi_{t}\left(y_{t}\right)
\end{equation}

\begin{equation}
\psi_{st}\left( y_{s},y_{t} \right)=exp\left( -\dfrac{1}{2} y_{s}\Delta_{st}y_{t} \right)
\end{equation}

\begin{equation}
\psi_{t}\left(y_{t}\right)= \exp \left( -\dfrac{1}{2}\Delta_{tt}y^{2}_{t}+\eta_{t}y_{t}\right)
\end{equation}

\begin{equation}
p\left(\textbf{y}|\boldsymbol{\theta}\right) \propto \exp \left( \boldsymbol{\eta}^{T} \textbf{y}-\dfrac{1}{2}y^{T}\Delta \textbf{y} \right)
\end{equation}

(this last expression can be reconducted to the multivariate gassian if one consider $\boldsymbol{\Delta}=\boldsymbol{\Sigma}^{-1}$ and $\boldsymbol{\eta}=\boldsymbol{\Delta}\boldsymbol{\mu}$. Given the network, we would now move on how the parameters can be achieved. Lets start from a Markov random field in log-linear form \cite{murphy2012machine}:

\begin{equation}
p\left(\textbf{y}|\boldsymbol{\theta}\right) = \dfrac{1}{Z(\theta)}\exp \left( \sum_{c}\boldsymbol{\theta}^{T}_{c}\phi_{c}\left(\textbf{y}\right)\right)
\end{equation}

thus we can define the log-likehood as \cite{murphy2012machine}:

\begin{equation}
\mathcal{L}\left(\boldsymbol{\theta}\right):= \frac{1}{N}\sum_{i}\log p\left(\textbf{y}_{i}|\boldsymbol{\theta}\right)=\frac{1}{N}\sum_{i}\left[\sum_{c} \boldsymbol{\theta}^{T}_{c}\phi_{c}(y_{i})-\log Z\left(\boldsymbol{\theta}\right)\right]
\end{equation}

\begin{equation}
\frac{\partial\mathcal{L}}{\partial\boldsymbol{\theta}_{c}}=\frac{1}{N}\sum_{i}\left[\phi_{c}(y_{i})-\frac{\partial}{\partial\boldsymbol{\theta}_{c}}\log Z(\boldsymbol{\theta})\right]
\end{equation}

\begin{equation}
\frac{\partial \log Z(\boldsymbol{\theta})}{\partial\boldsymbol{\theta}}=\mathbb{E}\left[\phi_{c}(\textbf{y})\right|\theta]=\sum_{\textbf{y}}\phi_{c}(\textbf{y})p(\textbf{y}|\boldsymbol{\theta})}
\end{equation}


\begin{equation}
\frac{\partial\mathcal{L}}{\partial\boldsymbol{\theta}_{c}}=\left[\frac{1}{N}\sum_{i}\phi_{c}(y_{i})\right]-\mathbb{E}\left[\phi_{c}(\textbf{y})\right]
\end{equation}

In the first term $\textbf{y}$ is fixed to its observed values while in the second it is free. Such expression can be recasted in to a more explicative form \cite{murphy2012machine}:

\begin{equation}
\frac{\partial\mathcal{L}}{\partial\boldsymbol{\theta}_{c}}=\mathbb{E}_{p_{emp}}\left[\phi_{c}(\textbf{y})\right]-\mathbb{E}_{p_{(\cdot|\boldsymbol{\theta})}}\left[\phi_{c}(\textbf{y})\right]
\end{equation}

Therfore at the optimum we will have \cite{murphy2012machine}:

\begin{equation}
\mathbb{E}_{p_{emp}}\left[\phi_{c}(\textbf{y})\right]=\mathbb{E}_{p_{(\cdot|\boldsymbol{\theta})}}\left[\phi_{c}(\textbf{y})\right]
\end{equation}

From this expression it is clear why this method is called moment matching; it is worth nothing tha such computation is largely expensive from a computational point of view: thus scholar usually consider other techinques or at least stochastic gradient descent method. A full review can be found in \cite{murphy2012machine} and \cite{koller2009probabilistic}. Finally we consider, as for the dataset in analysed in this work, the case where we have both discrete and continuous variables i.e. $x=\left(i_{1},...,i_{d},y_{1},...,y_{q} \right)$ with d discrete variable and q continuos variables. The are called in the literature Mixed Interaction Models. In this case the following density has to be considered \cite{hojsgaard2012graphical}:

\begin{equation}
\begin{split}
f(i,y)=& p(i)(2\pi)^{-q/2}det(\Sigma)^{-1/2} \\
& exp\left[-\dfrac{1}{2}\left(y-\mu(i)\right)^{T}\Sigma^{-1}\left(y-\mu(i)\right)\right]
\end{split}
\label{gaussMix}
\end{equation}

Which can be rewritten in the exponential family form \cite{hojsgaard2012graphical}: 

\begin{equation}
\begin{split}
f(i,y) & = \exp\left\lbrace g(i)+\sum_{u}h^{u}(i)y_{u}-\dfrac{1}{2}\sum_{uv} y_{u}y_{v}k_{uv}\right\rbrace \\
&= \exp\left\lbrace g(i)+h(i)^{T}y-\dfrac{1}{2}y^{T}Ky \right\rbrace
\end{split}
\end{equation}

where $g(i)$, $h(i)$ and $K$ are the canonical parameters. These are connected with the parameters of expression \ref{gaussMix} by the following identities \cite{hojsgaard2012graphical}: 

\begin{equation}
\begin{split}
K=&\Sigma^{-1} \\
h(i)=&\Sigma^{-1}\mu(i) \\
g(i)=&\log p(i) -\frac{1}{2}\log det (\Sigma) \\
&-\dfrac{1}{2}\mu(i)^{T}\Sigma^{-1}\mu(i)-\dfrac{q}{2}\log 2\pi
\end{split}
\end{equation}

Moreover one can further modify the previous form in order to obtain a particular factorial expansion: such models are referred as homogeneous mixed interaction models \cite{hojsgaard2012graphical}.

\section{Information theory}
Given an ensemble of random variable, we can find the amount of information that one variable contains of another one: such quantity is called mutual information and it is a key concept within the information theory. This approach, that was implemented by Claude Shannon decades before the probabilistic modelling, represent a complementary way by which one can attack the problem of conditional dependence between random variables. Here we would provide some basic concepts of this theory, following the Cover \cite{cover2006elements} and MacKay \cite{mackay2003information} approaches, that allows to properly define the concept of mutual information. The starting concept of information theory is the entropy which express the uncertainty of a random variable. Given a random variable $X$ with alphabet (the accessible states) $\cite{cover2006elements}:$ and probability mass function $p(x)=Pr\left\lbrace X=x \rbrace\; x \in \chi$ we define the entropy of $X$ as $H(X)=-\sum_{x \in X} p(x)log p(x)$ where the logarithm has to be considered with basis 2. In analogous way the joint entropy of two random variables $(X,Y)$ with a joint distribution p(x,y) is defined as \cite{cover2006elements}:
\begin{equation}
H(X,Y)=-\sum_{x\in \chi}\sum_{y\in \mathcal{Y}}p(x,y)\log p(x,y)
\end{equation}
Furthermore, we can define also the conditional entropy as \cite{cover2006elements}:
\begin{equation}
\begin{split}
H(X|Y)=& \sum_{x\in \mathcal{X} }p(x)H(Y|X=x)\\
=& -\sum_{x\in \mathcal{X}}p(x)\sum_{y\in \mathcal{Y}}p(x,y)\log p(y|x) \\
=& -E\log p(Y|X)
\end{split}
\end{equation}
The joint entropy and the conditional entropy are related by the chain rule \cite{cover2006elements}:
\begin{equation}
H(X,Y)=H(X)+H(Y|X)
\end{equation}
Such rule can be extended to to the following from \cite{cover2006elements}:
\begin{equation}
H(X,Y|Z)=H(X|Z)+H(Y|X,Z)
\end{equation}
Given a distribution q and another distribution p, one can quantify how inefficently the second one describe the first one using the concept of relative entropy or Kullback-Leibler distance \cite{cover2006elements,mackay2003information}:
\begin{equation}
D(p||q)=\sum p(x)\log\frac{p(x)}{q(x)}
\end{equation}
As stated by the Gibbs inequality \cite{cover2006elements,mackay2003information}:
\begin{equation}
D(p||q)\geq 0
\end{equation}
this quantity can not be negative: the entropy of a random variable associated to another cannot have a degree of uncertainty lower with respect to the quantity that its aimed to describe. On these basis we are now ready to introduce the concept of mutual information. This is defined as  \cite{cover2006elements}:
\begin{equation}
\begin{split}
I(X;Y)&=\sum\sump(x,y)\log\dfrac{p(x,y)}{p(x)p(y)}=\\
&=D(p(x,y)||p(x)p(y)) \\
&=H(X)-H(X|Y)=H(Y)-H(Y|X)
\end{split}
\end{equation}
As for the joint distribution also in this case we have a chain rule  \cite{cover2006elements}:
\begin{equation}
I(X_{1},X_{2},...,X_{n};Y)=\sum^{n}_{i=1}I(X_{i};Y|X_{i-1},X_{i-2},...,X_{1})
\end{equation}
Finally we would report the data process inequality theorem that connects the information theory with the Markov chain: if we have a Markov chains, $X\rightarrow Y \rightarrow Z$  then $I(X;Y)\geq I(X;Z) $. As for the Gibbs inequality, the underlying idea is that no clever manipulation of the data can improve the inference that can be made on them \cite{cover2006elements,mackay2003information}. Otherwise we would have a clear violation of the second principle of thermodynamics (see for instance the Maxwell's demon \cite{feynman2018feynman})


\chapter{Dataset description}

\chapter{Results}

This chapter is organized in the following way: first we are going to report the results concerning the preliminary analysis performed on the dataset. This include the factor analysis of mixed data and the mutual information analysis of the continuous variables of asteroids vs their hazard. Then it will follow the analysis of the dataset performed with the probabilistic methods: after a preliminary analysis on the continuous variables, the mixed interaction model as well the minforest model obtained for the whole dataset will be presented and discussed. Finally the probabilistic models previously obtained will be compared with the outputs and the performances of four machine learning algorithm (Random Forest, Support vector machines Quadratic Discriminant Analysis and Logistic regression).

\section{Preliminary analysis}

In Fig. \ref{FADM},\ref{FAMD_Quantitative variables} and \ref{FAMD_Individuals_(c)} are reported the main achivements of FADM (performed with FactoMineR package \cite{le2008factominer}): in particular from the correlation circle in Fig. \ref{FAMD_Quantitative variables} we can recognize the different correlations that are given by the celestial mechanics laws: for instance see that there is strong correlation of the mean motion with the semi-major axis. This is due to the Kepler law discussed in Appendix A. Furthermore we see that that the mean motion is correctly almost independent with respect to the the diameter (max or min) of  the asteroid. As explained in Appendix A this is an another result of Newton gravitation theory for a two body interaction: the mean motion of an asteroid, as long as the the mass of it is much lower with respect to the sun, is independent from its mass. On the other side the fact that relative velocity is correlated with the diameter is a spurios correlation. At this point one can ask why the mean motion and the relative velocity are orthogonal: this point will be clarified from a theoretical point of view in appendix A and also with graphical models. Briefly the motion of an object on an ellipse as seen from a focus is not uniforms: this is faster as the two bodies approaches. Beside this inspection a further analysis based on the concept of mutual information (summarized in the previous chapter) was performed: its result is reported in Fig. \ref{Mutual_information}. This figure summarize the ranking of the features considered in the dataset for the dangerousness classification of the asteroids according to the following expression \cite{kratzer2018varrank}

\begin{equation}
g(\alpha,\textbf{C},\textbf{S},f_{i})=MI(f_{i};\textbf{C})-\sum_{f_{s}\in S}\alpha(f_{i},f_{s},\textbf{C},\textbf{S})MI(f_{i};f_{s})
\end{equation}
where the first term $MI(f_{i};\textbf{C})$ is called relevance and measures the Mutual Information between the interesting feature set $\textbf{C}$ (only Hazardous in our case) and the analysed one $f_{i}$; the third term $MI(f_{i};f_{s})$ is called redundancy and measures the MI between the analysed feature and a chosen set of them. Finally the $\alpha(f_{i},f_{s},\textbf{C},\textbf{S})$ is a normalization function and in our case was set to \cite{kratzer2018varrank}

\begin{equation}
\alpha(f_{i},f_{s},\textbf{C},\textbf{S})=\dfrac{1}{|\textbf{S}|}
\end{equation}

following the Peng. et al approach \cite{peng2005feature}. We see that the first placein the mutual information ranking, looking to the diagonal element,  is taken by minimum orbit intersection: this is correct since this is one parameter used by NASA for deciding if an asteroid is hazardous or not (see Appendix A). The second place is occupied by Epoch date close approach. The third place is the eccentricity value: this parameter is entangled with the minimum orbit intersection and thus it is reasonable that it is important. Then we have two parameter that are related to the dimension of the asteroid: this is meaningful since if the asteroid has a too reduced volume it will be destroyed by the Earth atmosphere. On the other side the other parameters seems to have a too low MI for being interesting in this preliminary analysis. The results obtained for FAMD and MI will provide a useful path-guide, together with the celestial mechanics theory, for the interpretation of the results obtained in the following section 


\begin{figure}[h]
\begin{center}
\includegraphics[width=1\textwidth]{Figures/FAMD.pdf}
\caption{The FAMD main plot in which the correlation between the continuous and discrete variables is reported. Plot obtained from FactoMineR package \cite{le2008factominer}}
\label{FADM}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=1\textwidth]{Figures/FAMD_Quantitative variables.pdf}
\caption{The FAMD correlation circle for continuos variables as obtained from FactoMineR package \cite{le2008factominer}}
\label{FAMD_Quantitative variables}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=1\textwidth]{Figures/FAMD_Individuals_(c).pdf}
\caption{Graph of individuals, for the qualititative variables, as obtained from FactoMineR package \cite{le2008factominer}}
\label{FAMD_Individuals_(c)}
\end{center}
\end{figure}

\begin{landscape}
\begin{figure}
\begin{center}
\includegraphics[width=1.75\textheight]{Figures/Mutual_information.pdf}
\caption{Mutual information as obtained with the varrank package \cite{kratzer2018varrank}}
\label{Mutual_information}
\end{center}
\end{figure}
\end{landscape}

\pagebreak

\section{Probabilistic models}

We started the analysis with the graphical models by inspecting the relations between continuous variables in the dataset, thus in this first step the binary variable \textit{Hazardous} was excluded. For this purpose, among the different methods available, we considered the \textit{Graphical least absolute shrinkage and selection operator} GLASSO as implemented in the \textit{glasso} R package \cite{friedman2008sparse,glasso}. After different tests reported in Fig. \ref{GLASSO_convergence}, we considered as final result the graph obtained with the value of $\rho$ (the one that penalize further connections) equal to $0.3$. This is beacuse in this plot we see that the conditional dependences/independences state by the Celestial Mechanics are correctly reproduced: for instance we see that the diameter of the asteorids is indipendent from all features related to its motion. This is definitely meaningful since the mass of the asteroids is largely lower with respect to the mass of earth: thus there is no way by which the asteroid orbit can be modified by its mass as explained in Appendix A. Furthermore we see that the Close approach Date and the Epoch date are dependant each other but in no way from the other features: this is right since the date and epoch are set with an arbitary scale. Also the Perihelion Epoch and Osculation Time are dependant, as expected, but there is no depencdence with the orbit parameters. We see that the mean motion is conditionally indipendent with respect to relative velocity, but correctly this is dependent from perihelion distance.


\begin{figure}[ht]
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=.9\linewidth]{Figures/GLASSO_0.1.pdf}
    \subcaption{$\rho$=0.1}
    \vspace{4ex}
  \end{minipage} 
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=.9\linewidth]{Figures/GLASSO_0.2.pdf}
    \subcaption{$\rho$=0.2}
    \vspace{4ex}
  \end{minipage} \\
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=.9\linewidth]{Figures/GLASSO_0.3.pdf}
    \subcaption{$\rho$=0.3}
    \vspace{4ex}
  \end{minipage}%%
    \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=.9\linewidth]{Figures/GLASSO_0.4.pdf}
    \subcaption{$\rho$=0.4}
    \vspace{4ex}
  \end{minipage}%%
\caption{GLASSO analysis, performed with the glasso package \cite{friedman2008sparse,glasso}, with different $\rho$ parameter (the one that penalize further connections) for the Asteroid dataset without the discrete variable Hazardous. The plots were obtained with the igraph package for R \cite{igraph}}
\label{GLASSO_convergence}
\end{figure}

Lets now move on the mixed interaction model by which  



\section{Machine learning algorithms}

\chapter{Conclusions}

\chapter{Appendix A: Concepts of celestial mechanics}


\bibliographystyle{unsrt}

\bibliography{sample.bib} % The file containing the bibliography

\newpage




%----------------------------------------------------------------------------------------

\end{document}
